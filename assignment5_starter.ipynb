{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.features_train = np.array(X)\n",
    "        self.labels_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        #batch processing for system memory reasons\n",
    "        batch_size = 1000\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = []\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_X = X[start:end]\n",
    "            distances = self.compute_distance(batch_X)  # Compute distances in batches\n",
    "\n",
    "            knearest = np.argpartition(distances, self.k, axis=1)[:, :self.k]  # Get the k nearest neighbors (indices)\n",
    "            nearestLabels = self.labels_train[knearest].astype(int)  # Get the labels of the k-nearest neighbors\n",
    "\n",
    "            # Majority vote to determine the prediction for each sample\n",
    "            batch_predictions = np.array([np.bincount(labels).argmax() for labels in nearestLabels])\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    #predict method but with the probaility estimates not just the classification\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        distances = self.compute_distance(X)\n",
    "        knearest = np.argsort(distances, axis=1)[:, :self.k]\n",
    "        nearestLabels = self.labels_train[knearest]\n",
    "            \n",
    "        #probability\n",
    "        prob_class_1 = np.sum(nearestLabels == 1, axis=1) / self.k\n",
    "        probabilities = np.column_stack((1 - prob_class_1, prob_class_1))        \n",
    "        return probabilities  # Return probabilities as an array\n",
    "            \n",
    "    def compute_distance(self, X):\n",
    "        # TODO: Implement distance computation based on self.distance_metric\n",
    "        # Hint: Use numpy operations for efficient computation\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            squared_diff = ((X[:, np.newaxis, :] - self.features_train[np.newaxis, :, :]) ** 2)\n",
    "            return np.sqrt(squared_diff.sum(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data preprocessing function\n",
    "\n",
    "# weight the features based on correlation\n",
    "def get_feature_weights(X, y):\n",
    "    n_features = X.shape[1]\n",
    "    correlations = np.zeros(n_features)\n",
    "    for i in range(n_features):\n",
    "        corr = np.abs(np.corrcoef(X[:, i], y)[0, 1])\n",
    "        correlations[i] = corr\n",
    "    return correlations  \n",
    "\n",
    "def preprocess_data(train_path, test_path):\n",
    "    print(\"preprocess begins\")\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    print(\"csv read\")\n",
    "    \n",
    "    \n",
    "    # Drop columns which don't contribute any meaningful info\n",
    "    columns_to_drop = ['id', 'CustomerId', 'Surname', 'EstimatedSalary', 'HasCrCard', 'Tenure', 'CreditScore' ]  # Removing columns that have a low correlation based on a seperate script I ran to calc correlations\n",
    "    train_data = train_data.drop(columns=columns_to_drop, axis=1)\n",
    "    test_data = test_data.drop(columns=columns_to_drop, axis=1)\n",
    "    \n",
    "    \n",
    "    X_train = train_data.drop('Exited', axis=1)  # drop label column\n",
    "    y_train = train_data['Exited']\n",
    "    X_test = test_data\n",
    "    \n",
    "    #turn categorical values into numerical ones\n",
    "    X_train = pd.get_dummies(X_train, columns=['Geography', 'Gender'], drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, columns=['Geography', 'Gender'], drop_first=True)\n",
    "    \n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "        \n",
    "    print(\"data processed and weighted\")\n",
    "    \n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    \n",
    "    return X_train_np, y_train, X_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function\n",
    "\n",
    "#help function for the ROC AUC\n",
    "def compute_roc_auc(y_true, y_probs):\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_probs = np.array(y_probs)\n",
    "    sorted_indices = np.argsort(y_probs)[::-1]\n",
    "    y_true = y_true[sorted_indices]\n",
    "    y_probs = y_probs[sorted_indices]\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    n_pos = np.sum(y_true)  \n",
    "    n_neg = len(y_true) - n_pos  \n",
    "    tp = 0  \n",
    "    fp = 0  \n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1: \n",
    "            tp += 1\n",
    "        else:  \n",
    "            fp += 1\n",
    "        tpr = tp / n_pos \n",
    "        fpr = fp / n_neg  \n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    auc = np.trapz(tpr_list, fpr_list)\n",
    "    return auc\n",
    "\n",
    "#balanced fold splitting\n",
    "def stratified_k_fold_split(X, y, n_splits=5):\n",
    "    y = np.array(y)\n",
    "    unique_classes, y_indices = np.unique(y, return_inverse=True)\n",
    "    class_counts = np.bincount(y_indices)\n",
    "    folds = [[] for _ in range(n_splits)]\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        np.random.shuffle(cls_indices)\n",
    "        cls_fold_sizes = np.full(n_splits, len(cls_indices) // n_splits)\n",
    "        cls_fold_sizes[:len(cls_indices) % n_splits] += 1\n",
    "        start = 0\n",
    "        for fold_idx, fold_size in enumerate(cls_fold_sizes):\n",
    "            end = start + fold_size\n",
    "            folds[fold_idx].extend(cls_indices[start:end])\n",
    "            start = end\n",
    "\n",
    "    return folds\n",
    "\n",
    "#cross validation\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    np.random.seed(42) \n",
    "    #set up folds using stratified folds to ensure correct splits\n",
    "    folds = stratified_k_fold_split(X, y, n_splits)\n",
    "    auc_scores = []\n",
    "    for i in range(n_splits):\n",
    "        test_indices = folds[i]\n",
    "        train_indices = np.hstack([folds[j] for j in range(n_splits) if j != i])\n",
    "        \n",
    "        X_train_fold, y_train_fold = X[train_indices], y[train_indices]\n",
    "        X_valid_fold, y_valid_fold = X[test_indices], y[test_indices]\n",
    "        \n",
    "        \n",
    "        feature_weights = get_feature_weights(X_train_fold, y_train_fold)\n",
    "        max_weight = np.max(feature_weights)\n",
    "        feature_weights = feature_weights / max_weight\n",
    "        \n",
    "        #scale seperately for each fold to avoid accidently applying data which shouldn't be avalaible\n",
    "        means = X_train_fold.mean(axis=0)\n",
    "        stds = X_train_fold.std(axis=0)\n",
    "        stds[stds == 0] = 1\n",
    "\n",
    "        # Scale training and validation data\n",
    "        X_train_fold_scaled = (X_train_fold - means) / stds\n",
    "        X_valid_fold_scaled = (X_valid_fold - means) / stds\n",
    "        \n",
    "        #weight scaled data\n",
    "        X_train_fold_weighted = X_train_fold_scaled * feature_weights\n",
    "        X_valid_fold_weighted = X_valid_fold_scaled * feature_weights\n",
    "        \n",
    "        # Train and evaluate the model\n",
    "        knn.fit(X_train_fold_scaled, y_train_fold)\n",
    "        y_probs = knn.predict_proba(X_valid_fold_scaled)[:, 1]\n",
    "        auc_score = compute_roc_auc(y_valid_fold, y_probs)\n",
    "        auc_scores.append(auc_score)\n",
    "    \n",
    "    # Return the average AUC score across all folds\n",
    "    return np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting preprocess\n",
      "preprocess begins\n",
      "csv read\n",
      "data processed and weighted\n",
      "done with preprocess\n",
      "Starting cross validation\n",
      "K: 1\n",
      "Score: 0.7057555705063823\n",
      "K: 2\n",
      "Score: 0.8011159663473872\n",
      "K: 3\n",
      "Score: 0.8497125872796436\n",
      "K: 4\n",
      "Score: 0.8723657465626042\n",
      "K: 5\n",
      "Score: 0.8801135232694657\n",
      "K: 6\n",
      "Score: 0.8911781097440828\n",
      "K: 7\n",
      "Score: 0.9008754655405289\n",
      "K: 8\n",
      "Score: 0.9023092531464385\n",
      "K: 9\n",
      "Score: 0.9047624110979726\n",
      "K: 10\n",
      "Score: 0.9075603404354723\n",
      "K: 11\n",
      "Score: 0.90980393796474\n",
      "K: 12\n",
      "Score: 0.9115029404426209\n",
      "K: 13\n",
      "Score: 0.9129114707284112\n",
      "K: 14\n",
      "Score: 0.9130968206381762\n",
      "K: 15\n",
      "Score: 0.9138915474916379\n",
      "K: 16\n",
      "Score: 0.9145265133795771\n",
      "K: 17\n",
      "Score: 0.9155487769814211\n",
      "K: 18\n",
      "Score: 0.9165251436336405\n",
      "K: 19\n",
      "Score: 0.9177439876029473\n",
      "K: 20\n",
      "Score: 0.9176366666196897\n",
      "K: 21\n",
      "Score: 0.9189590426811419\n",
      "K: 23\n",
      "Score: 0.9200648583773166\n",
      "K: 25\n",
      "Score: 0.9209266883211178\n",
      "K: 27\n",
      "Score: 0.9211609265330336\n",
      "K: 29\n",
      "Score: 0.9226109504295337\n",
      "K: 31\n",
      "Score: 0.922155452471749\n",
      "K: 33\n",
      "Score: 0.921405512809099\n",
      "K: 35\n",
      "Score: 0.9225535945835395\n",
      "K: 37\n",
      "Score: 0.9223525481731067\n",
      "K: 39\n",
      "Score: 0.9222970293109404\n",
      "K: 41\n",
      "Score: 0.9221104690951574\n",
      "K: 43\n",
      "Score: 0.9216462120590236\n",
      "K: 45\n",
      "Score: 0.921635302323392\n",
      "K: 47\n",
      "Score: 0.9222526207236053\n",
      "K: 49\n",
      "Score: 0.9219922942184902\n",
      "K: 51\n",
      "Score: 0.9216821827832167\n",
      "K: 53\n",
      "Score: 0.9217919987510973\n",
      "K: 55\n",
      "Score: 0.9216634032752978\n",
      "K: 57\n",
      "Score: 0.9216739364574608\n",
      "K: 59\n",
      "Score: 0.9210423885794234\n",
      "K: 61\n",
      "Score: 0.9208122624099456\n",
      "K: 63\n",
      "Score: 0.9210837906783389\n",
      "K: 65\n",
      "Score: 0.9207756852825234\n",
      "K: 67\n",
      "Score: 0.9210198922381789\n",
      "K: 69\n",
      "Score: 0.9204432897564458\n",
      "K: 71\n",
      "Score: 0.9203605677166775\n",
      "K: 73\n",
      "Score: 0.920337171955428\n",
      "K: 75\n",
      "Score: 0.9201302508503169\n",
      "K: 77\n",
      "Score: 0.9201568688200652\n",
      "K: 79\n",
      "Score: 0.920127955419181\n",
      "K: 81\n",
      "Score: 0.9197501272885464\n",
      "K: 83\n",
      "Score: 0.9197445654587082\n",
      "K: 85\n",
      "Score: 0.9195374180241561\n",
      "K: 87\n",
      "Score: 0.9195286012363691\n",
      "K: 89\n",
      "Score: 0.9196664577765823\n",
      "K: 91\n",
      "Score: 0.91933356794286\n",
      "K: 93\n",
      "Score: 0.9191468664735432\n",
      "K: 95\n",
      "Score: 0.9189577132109976\n",
      "K: 97\n",
      "Score: 0.9186265916505242\n",
      "K: 99\n",
      "Score: 0.918864241988819\n",
      "K: 101\n",
      "Score: 0.9185618279890507\n",
      "Optimal k value: 29 with cross-validation score: 0.9226109504295337\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "print(\"starting preprocess\")\n",
    "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "print(\"done with preprocess\")\n",
    "\n",
    "k_values = list(range(1, 21)) + list(range(21, 102, 2))\n",
    "\n",
    "best_k = None\n",
    "best_score = -1\n",
    "print(\"Starting cross validation\")\n",
    "for k in k_values:\n",
    "    # Create and evaluate model\n",
    "    knn = KNN(k=k, distance_metric='euclidean')\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_validate(X, y, knn)\n",
    "    print(\"K:\", k)\n",
    "    print(\"Score:\", cv_scores)\n",
    "    #keep track of the best performing k val\n",
    "    if cv_scores > best_score:\n",
    "        best_score = cv_scores\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Optimal k value: {best_k} with cross-validation score: {best_score}\")\n",
    "\n",
    "# compute feature weights on the full training data\n",
    "feature_weights = get_feature_weights(X, y)\n",
    "max_weight = np.max(feature_weights)\n",
    "feature_weights = feature_weights / max_weight  # normalize weights\n",
    "means = X.mean(axis=0)\n",
    "stds = X.std(axis=0)\n",
    "stds[stds == 0] = 1\n",
    "\n",
    "# scale and weight the features\n",
    "X_scaled = ((X - means) / stds) * feature_weights\n",
    "X_test_scaled = ((X_test - means) / stds) * feature_weights\n",
    "\n",
    "# train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "knn = KNN(k=best_k, distance_metric='euclidean')\n",
    "knn.fit(X_scaled, y)\n",
    "test_predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "# Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
